{"cells":[{"cell_type":"code","execution_count":null,"id":"15059747","metadata":{"id":"15059747"},"outputs":[],"source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"id":"5Pr4hNilpEqc","metadata":{"id":"5Pr4hNilpEqc"},"outputs":[],"source":["#setting base directories\n","basedir = \"/content/drive/My Drive/MSc Research\"\n","notebook_directory = basedir + \"/notebooks\"\n","data_directory = basedir + \"/data\"\n","pdb_directory = basedir + \"/pdb\"\n","installs_directory = basedir + \"/installs\"\n","alignment_directory = data_directory + \"/alignments\"\n","tmp_dir = alignment_directory + \"/tmp/\"\n","signal_p_directory = installs_directory + \"/signalp-5.0b\"\n","os.environ['LIBGL_ALWAYS_INDIRECT'] = '1'"]},{"cell_type":"code","execution_count":null,"id":"g3RV6AYqMoJX","metadata":{"collapsed":true,"id":"g3RV6AYqMoJX"},"outputs":[],"source":["#pip installs\n","!apt-get update\n","!pip install biopython\n","!pip install logomaker\n","!pip install py3Dmol\n","!pip install catboost"]},{"cell_type":"code","execution_count":null,"id":"_TS9p0WPjnNn","metadata":{"collapsed":true,"id":"_TS9p0WPjnNn"},"outputs":[],"source":["#installs\n","os.chdir(installs_directory)\n","!apt-get install muscle\n","!apt-get install -y cmake libfftw3-dev libomp-dev\n","\n"]},{"cell_type":"code","source":["\n","%load_ext pycodestyle_magic"],"metadata":{"id":"FPmy0BHMChzm"},"id":"FPmy0BHMChzm","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"56230353-8457-4ac5-b88f-d02588d3b794","metadata":{"collapsed":true,"id":"56230353-8457-4ac5-b88f-d02588d3b794"},"outputs":[],"source":["\n","import re\n","import csv\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc, roc_auc_score\n","from Bio.Seq import Seq\n","from Bio.SeqRecord import SeqRecord\n","import logomaker\n","from sklearn.metrics import classification_report, accuracy_score\n","from sklearn import svm\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.feature_selection import SelectFpr, f_classif\n","from sklearn.svm import SVC\n","from io import StringIO\n","from Bio import AlignIO, SeqIO\n","import logging\n","import seaborn as sns\n","from Bio import pairwise2\n","from Bio.pairwise2 import format_alignment\n","import cProfile\n","from IPython.utils import io\n","import tqdm.notebook\n","import os\n","import py3Dmol\n","from concurrent.futures import ThreadPoolExecutor\n","import multiprocessing\n","import subprocess\n","import shlex\n","import shutil\n","import stat\n","from collections import Counter\n","from ast import literal_eval\n","import io\n","import base64\n","from PIL import Image\n","from sklearn.ensemble import RandomForestClassifier\n","import fnmatch\n","from sklearn.model_selection import cross_val_score\n","import xgboost as xgb\n","from sklearn.linear_model import LogisticRegression\n","from imblearn.over_sampling import SMOTE\n","\n"]},{"cell_type":"code","execution_count":null,"id":"IhA1P8oypxDc","metadata":{"id":"IhA1P8oypxDc"},"outputs":[],"source":["os.chdir(data_directory)\n"]},{"cell_type":"code","execution_count":null,"id":"0wAuR23UaAIa","metadata":{"id":"0wAuR23UaAIa"},"outputs":[],"source":["#method to align sequences from fasta files\n","def align_sequences(input_fasta, input_fasta_2 = '', mode = 'msa'):\n","\n","\n","  if mode == 'msa':\n","    cmd = [\n","          'muscle', '-diags', '-maxiters', '3', '-sv', '-in', input_fasta\n","          ]\n","  else:\n","    print(\"Invalid mode.\")\n","    return\n","\n","  try:\n","    result = subprocess.run(cmd, capture_output=True, text=True)\n","  except subprocess.CalledProcessError as e:\n","    print(f\"Error running MUSCLE: {e}\")\n","    return\n","  if result.stderr:\n","    print(\"Error: \" + result.stderr)\n","  return result.stdout\n"]},{"cell_type":"code","execution_count":null,"id":"m0DrhdeO387h","metadata":{"id":"m0DrhdeO387h"},"outputs":[],"source":["#method to get the sequences as a file\n","def get_as_file(contents, file):\n","    print('file: ' + file)\n","    try:\n","        with open(file, 'w') as f:\n","            f.write(contents)\n","    except Exception as e:\n","        print(f\"Error writing alignment to file: {e}\")\n","    return file"]},{"cell_type":"code","execution_count":null,"id":"bFcW8Ks7fO-O","metadata":{"id":"bFcW8Ks7fO-O"},"outputs":[],"source":["\"\"\"Replace ambiguous characters with a gap or remove them.\"\"\"\n","def clean_sequence(sequence):\n","    return Seq(str(sequence).replace('X', '-').replace('B', '-').replace('J', '-').replace('Z', '-').replace('.', '-').replace('*', '-'))\n"]},{"cell_type":"code","execution_count":null,"id":"axveq7MCVGdZ","metadata":{"id":"axveq7MCVGdZ"},"outputs":[],"source":["\"\"\"Clean ambiguous characters in sequences before alignment.\"\"\"\n","def preprocess_fasta(input_fasta):\n","    temp_fasta = input_fasta + '.tmp'\n","    cleaned_records = []\n","    for record in SeqIO.parse(input_fasta, \"fasta\"):\n","        record.seq = clean_sequence(record.seq)\n","        cleaned_records.append(record)\n","\n","    SeqIO.write(cleaned_records, temp_fasta, \"fasta\")\n","\n","    os.replace(temp_fasta, input_fasta)\n","\n","    return input_fasta"]},{"cell_type":"code","execution_count":null,"id":"fx1IANJ6q-JY","metadata":{"id":"fx1IANJ6q-JY"},"outputs":[],"source":["def process_alignment(input_fasta, output_fasta, mode='msa', input_fasta_2 = ''):\n","    res_file = None\n","    input_fasta = preprocess_fasta(input_fasta)\n","    aligned_seq = align_sequences(input_fasta, input_fasta_2=input_fasta_2, mode=mode)\n","    if aligned_seq:\n","      print('Alignment successful.')\n","      res_file = get_as_file(aligned_seq, output_fasta)\n","    else:\n","        print('Error: sequence alignment failed.')\n","\n","    return res_file"]},{"cell_type":"code","execution_count":null,"id":"OeEeiR1mp_R1","metadata":{"id":"OeEeiR1mp_R1"},"outputs":[],"source":["#method for multiple sequence alignment\n","def msa(input_files, output_files, mode='msa'):\n","  res = list()\n","  for i in range(len(input_files)):\n","    out = None\n","    try:\n","      out = process_alignment(input_files[i], output_files[i])\n","    except Exception as e:\n","      logging.error(f\"Error processing {input_files[i]}: {e}\")\n","    res.append(out)\n","  return res"]},{"cell_type":"code","execution_count":null,"id":"s4vfv8-HL5vp","metadata":{"id":"s4vfv8-HL5vp"},"outputs":[],"source":["#method for calculating conservation scores of alignments\n","def calculate_conservation_score(fasta_file):\n","    \"\"\"\n","    Calculate a simple conservation score for each column in the alignment.\n","    The score is the fraction of the most common residue in the column.\n","    \"\"\"\n","    alignment = AlignIO.read(fasta_file, \"fasta\")\n","    conservation_scores = []\n","    for i in range(alignment.get_alignment_length()):\n","        column = alignment[:, i]  # Get the i-th column\n","        counts = Counter(column)  # Count the occurrences of each residue\n","        most_common_residue, most_common_count = counts.most_common(1)[0]\n","        score = most_common_count / len(column)  # Calculate conservation score\n","        conservation_scores.append(score)\n","\n","    return conservation_scores\n"]},{"cell_type":"code","source":["#method for calculating scores of alignments\n","def calculate_alignment_score(fasta_file):\n","    \"\"\"\n","    Calculate the alignment score based on matches and gaps.\n","    A higher score indicates better alignment quality.\n","    \"\"\"\n","    alignment = AlignIO.read(fasta_file, \"fasta\")\n","    score = 0\n","    total_columns = alignment.get_alignment_length()\n","\n","    for i in range(total_columns):\n","        column = alignment[:, i]\n","        matches = column.count(column[0])\n","        gaps = column.count('-')\n","        score += matches - gaps\n","\n","    return score / total_columns"],"metadata":{"id":"0b9QMGDMXk7J"},"id":"0b9QMGDMXk7J","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"NMku_OSl9pI-","metadata":{"id":"NMku_OSl9pI-"},"outputs":[],"source":["#method to combine sequences from multiple files in a single file\n","def combine_sequences(files, combined_file):\n","    \"\"\"\n","    combine avian and human files under one file per subtype\n","    \"\"\"\n","    with open(combined_file, 'a') as outfile:\n","        for file in files:\n","            with open(file, 'r') as infile:\n","                outfile.write(infile.read())"]},{"cell_type":"code","execution_count":null,"id":"Py2gxCRI1BKd","metadata":{"id":"Py2gxCRI1BKd"},"outputs":[],"source":["h3, h5, h7 = \"h3.fasta\", \"h5.fasta\", \"h7.fasta\""]},{"cell_type":"code","execution_count":null,"id":"p6QV9Uo65j5i","metadata":{"id":"p6QV9Uo65j5i"},"outputs":[],"source":["h3_human, h5_human, h7_human = \"h3_human.fasta\", \"h5_human.fasta\", \"h7_human.fasta\"\n","h3_avian, h5_avian, h7_avian = \"h3_avian.fasta\", \"h5_avian.fasta\", \"h7_avian.fasta\""]},{"cell_type":"code","execution_count":null,"id":"T9kaAUbBqLdH","metadata":{"id":"T9kaAUbBqLdH"},"outputs":[],"source":["h3_pre = alignment_directory + \"/pre/\" + h3\n","h5_pre = alignment_directory + \"/pre/\" + h5\n","h7_pre = alignment_directory + \"/pre/\" + h7"]},{"cell_type":"code","execution_count":null,"id":"ztqvSX1UrzoZ","metadata":{"id":"ztqvSX1UrzoZ"},"outputs":[],"source":["#combine sequences for all hosts in a single subtype file for alignment\n","combine_sequences([alignment_directory + \"/pre/\" + h3_human, alignment_directory + \"/pre/\" + h3_avian], h3_pre)\n","combine_sequences([alignment_directory + \"/pre/\" + h5_human, alignment_directory + \"/pre/\" + h5_avian], h5_pre)\n","combine_sequences([alignment_directory + \"/pre/\" + h7_human, alignment_directory + \"/pre/\" + h7_avian], h7_pre)"]},{"cell_type":"code","execution_count":null,"id":"JQi9rjdTJF7p","metadata":{"id":"JQi9rjdTJF7p"},"outputs":[],"source":["h3_aligned = alignment_directory + \"/msa/\" + h3\n","h5_aligned = alignment_directory + \"/msa/\" + h5\n","h7_aligned = alignment_directory + \"/msa/\" + h7"]},{"cell_type":"code","execution_count":null,"id":"eoR7RqV6PNYb","metadata":{"colab":{"background_save":true},"collapsed":true,"id":"eoR7RqV6PNYb"},"outputs":[],"source":["#if no alignment files are available in the directory, use code in this cell\n","h3_aligned, h5_aligned, h7_aligned = msa([alignment_directory+ \"/pre/\" + h3, alignment_directory+ \"/pre/\" + h5, alignment_directory+ \"/pre/\" + h7], [h3_aligned, h5_aligned, h7_aligned])\n"]},{"cell_type":"code","execution_count":null,"id":"TRqHG2rSMLo3","metadata":{"collapsed":true,"id":"TRqHG2rSMLo3"},"outputs":[],"source":["#method to test alignment accuracy based on conservation and alignment scores\n","def test_alignment_accuracy(alignment_file):\n","  scores = calculate_conservation_score(alignment_file)\n","\n","  average_score = sum(scores) / len(scores)\n","  print(f\"\\nAverage Conservation Score: {average_score:.2f}\")\n","\n","  alignment_score = calculate_alignment_score(alignment_file)\n","  print(f\"\\nAlignment Score: {alignment_score:.2f}\")\n"]},{"cell_type":"code","execution_count":null,"id":"LVJoFGaZ07k5","metadata":{"id":"LVJoFGaZ07k5"},"outputs":[],"source":["#method to check if fasta is aligned\n","def is_fasta_aligned(input_fasta, dir):\n","  is_aligned = False\n","  with open(os.path.join(dir, input_fasta), 'r') as file:\n","    alignment = AlignIO.read(file, 'fasta')\n","    alignment_lengths = [len(record.seq) for record in alignment]\n","    is_aligned = len(set(alignment_lengths)) == 1\n","\n","  return is_aligned\n"]},{"cell_type":"code","execution_count":null,"id":"QTIcuV951feN","metadata":{"id":"QTIcuV951feN"},"outputs":[],"source":["#method for signal peptide detection\n","def run_signalp(fasta_dir, fasta_file):\n","\n","  signalp_output = \"\"\n","\n","  original_dir = os.getcwd()\n","\n","  signalp_executable = os.path.join(signal_p_directory, 'bin')\n","\n","  os.chmod(signalp_executable  + \"/signalp\", stat.S_IRWXU)\n","\n","  os.chdir(signalp_executable)\n","\n","  shutil.copy(os.path.join(fasta_dir, fasta_file), os.path.join(signalp_executable, fasta_file))\n","\n","  fasta_path = os.path.join(fasta_dir, fasta_file)\n","\n","  command = f'./signalp -fasta \"{fasta_file}\" -format short -gff3 -prefix output'\n","\n","  output_dir = os.path.join(signalp_executable, \"output.gff3\")\n","\n","  result = subprocess.run(shlex.split(command), capture_output=True, text=True)\n","\n","\n","  if result.returncode != 0:\n","    print(\"SignalP did not run successfully.\")\n","    print(result.stderr)\n","    exit(1)\n","  else:\n","    with open(output_dir, 'r') as f:\n","      signalp_output = f.read()\n","\n","  os.remove(os.path.join(signalp_executable, fasta_file))\n","  os.chdir(original_dir)\n","  os.remove(output_dir)\n","  return signalp_output\n"]},{"cell_type":"code","execution_count":null,"id":"6284xcx_VTjB","metadata":{"id":"6284xcx_VTjB"},"outputs":[],"source":["#get signal peptide proteins for all subtypes\n","h3_signal = run_signalp(alignment_directory + \"/msa/\", h3)\n","h5_signal = run_signalp(alignment_directory + \"/msa/\", h5)\n","h7_signal = run_signalp(alignment_directory + \"/msa/\", h7)"]},{"cell_type":"code","execution_count":null,"id":"GBaW9xtP00pA","metadata":{"id":"GBaW9xtP00pA"},"outputs":[],"source":["#remove n terminals of proteins based on detected signal peptide indices\n","def remove_n_terminal(signalp_output, input_fasta, output_fasta):\n","    crop_positions = {}\n","    for line in signalp_output.strip().split('\\n'):\n","        if line.startswith('#'):\n","            continue\n","        parts = line.split('\\t')\n","        if len(parts) >= 9:\n","            sequence_id = parts[0]\n","            start = int(parts[3]) - 1\n","            end = int(parts[4])\n","            crop_positions[sequence_id] = (start, end)\n","\n","    with open(input_fasta, \"r\") as handle:\n","        fasta_sequences = list(SeqIO.parse(handle, \"fasta\"))\n","\n","    # Crop the sequences\n","    for seq_record in fasta_sequences:\n","        seq_id = seq_record.id\n","        if seq_id in crop_positions:\n","            start, end = crop_positions[seq_id]\n","            seq_record.seq = seq_record.seq[end:]\n","\n","    with open(output_fasta, \"w\") as output_handle:\n","        SeqIO.write(fasta_sequences, output_handle, \"fasta\")\n","\n","    return output_fasta"]},{"cell_type":"code","execution_count":null,"id":"ljj_PJNqPLEl","metadata":{"id":"ljj_PJNqPLEl"},"outputs":[],"source":["h3_cropped = alignment_directory + \"/signalp\" + \"/\" + h3\n","h5_cropped = alignment_directory + \"/signalp\" + \"/\" + h5\n","h7_cropped= alignment_directory + \"/signalp\" + \"/\" + h7\n"]},{"cell_type":"code","execution_count":null,"id":"wn9Ldsb4VStN","metadata":{"collapsed":true,"id":"wn9Ldsb4VStN"},"outputs":[],"source":["#if mature proteins not available in hx_cropped directories, run cell to cleave proteins\n","remove_n_terminal(h3_signal, h3_aligned, h3_cropped)\n","remove_n_terminal(h5_signal, h5_aligned, h5_cropped)\n","remove_n_terminal(h7_signal, h7_aligned, h7_cropped)"]},{"cell_type":"code","execution_count":null,"id":"1OpcpG8dfkUL","metadata":{"id":"1OpcpG8dfkUL"},"outputs":[],"source":["h3_realigned = alignment_directory + \"/msa2/\" + h3\n","h5_realigned = alignment_directory + \"/msa2/\" + h5\n","h7_realigned = alignment_directory + \"/msa2/\" + h7\n"]},{"cell_type":"code","execution_count":null,"id":"0YtGRm3BDkwP","metadata":{"collapsed":true,"id":"0YtGRm3BDkwP"},"outputs":[],"source":["#realign cleaved proteins\n","msa([h3_cropped, h5_cropped, h7_cropped], [h3_realigned, h5_realigned, h7_realigned])"]},{"cell_type":"code","execution_count":null,"id":"t8VZitmeVFPR","metadata":{"id":"t8VZitmeVFPR"},"outputs":[],"source":["#method to extract subtype and year details from the header of a given sample\n","def get_header_details(header):\n","  subtype = []\n","  year = []\n","  host = []\n","  if (len(header) <= 0):\n","    print(\"header empty.\")\n","  else:\n","    parts = header.split(\"|\")\n","    if len(parts) >= 3:\n","        date = parts[0].strip()\n","        subtype = parts[1].strip().replace('_', '').replace('/', '').replace('A', '')\n","        year = date.split('-')[0] if '-' in date else []\n","        host = parts[2].strip()\n","    else:\n","        print(\"Invalid header format.\")\n","  return host, subtype, year\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"QbVvjtkoPE0U","metadata":{"id":"QbVvjtkoPE0U"},"outputs":[],"source":["#method to add the host sequence data provided in sequence_data to the dataframe given\n","def get_df_from_sequences(df, fasta_file):\n","    required_columns = ['label', 'year', 'subtype', 'sequence']\n","    sequences = []\n","    data = []\n","    if not all(col in df.columns for col in required_columns):\n","        print(\"Dataframe missing necessary columns. Adding...\")\n","        df = pd.DataFrame(columns=required_columns)\n","    if not fasta_file:\n","        print(\"Fasta file not found.\")\n","        return df\n","\n","    with open(fasta_file, 'r') as file:\n","        alignment = AlignIO.read(file, 'fasta')\n","        for record in alignment:\n","        #each sample stores sequence data following an isolate id in the header\n","          isolate_id_pattern = r\"(.*?)(EPI_ISL_\\d+)([\\w-]*)\"\n","          matches = re.findall(isolate_id_pattern, record.id, re.DOTALL)\n","          if not matches:\n","              continue\n","          header, isolate_id, sequence = matches[0]\n","          sequence = str(record.seq).strip().replace(\"\\n\", \"\")\n","          if len(sequence) > 0:\n","                host, subtype, year = get_header_details(header.strip())\n","                data.append((get_label(host), year, subtype, sequence))\n","          else:\n","              print(\"Sequence not found for \" + isolate_id)\n","\n","    new_df = pd.DataFrame(data, columns=required_columns)\n","    df = pd.concat([df, new_df], ignore_index=True)\n","\n","    return df"]},{"cell_type":"code","execution_count":null,"id":"eNhzwlJ-DA9Y","metadata":{"id":"eNhzwlJ-DA9Y"},"outputs":[],"source":["#'Equivalent amino acid numbering for subtypes currently circulating in humans or have pandemic potential.' by Burke & Smith (2014)\n","# (within the binding region)\n","data = [\n","        ('H', 'Y', 110),\n","        ('S', 'N', 126),\n","        ('S', 'P', 128),\n","        ('S', 'A', 137),\n","        ('A', 'V', 138),\n","        ('G', 'R', 143),\n","        ('I', 'T', 155),\n","        ('N', 'D', 158),\n","        ('T', 'A', 160),\n","        ('N', 'K', 186),\n","        ('D', 'G', 187),\n","        ('E', 'G', 190),\n","        ('T', 'I', 192),\n","        ('K', 'R', 193),\n","        ('Q', 'R or H', 196),\n","        ('V','I', 214),\n","        ('Q', 'L', 226),\n","        ('S', 'N', 227),\n","        ('G', 'S', 228),\n","        ('P', 'S', 239)]\n","\n","known_mutations_df = pd.DataFrame(data, columns = ['original', \"mutated\", 'H3 index'])\n"]},{"cell_type":"code","execution_count":null,"id":"KaSGxnY698gh","metadata":{"id":"KaSGxnY698gh"},"outputs":[],"source":["#corresponding indices accros H3, H5 and H7 data types (Burke & Smith, 2014)\n","numbering_data = [\n"," (110, 103, 100),\n"," (126, 121, 116),\n"," (128, 123, 118),\n"," (137, 133, 127),\n"," (138, 134, 128),\n"," (143, 139, 132),\n"," (155, 151, 144),\n"," (158, 154, 147),\n"," (160, 156, 151),\n"," (186, 182, 177),\n"," (187, 183, 178),\n"," (190, 186, 181),\n"," (192, 188, 183),\n"," (193, 189, 184),\n"," (196, 192, 187),\n"," (197, 193, 188),\n"," (214, 210, 205),\n"," (226, 222, 217),\n"," (227, 223, 218),\n"," (228, 224, 219),\n"," (239, 235, 230)]\n","\n","df_numbering_scheme = pd.DataFrame(data = numbering_data, columns = ['H3', 'H5', 'H7'])\n","\n"]},{"cell_type":"code","execution_count":null,"id":"wdT0cjjRotP5","metadata":{"id":"wdT0cjjRotP5"},"outputs":[],"source":["#method to get the index of a subtype based on its h3 index\n","def get_subtype_index(subtype, h3_index):\n","  subtype = subtype.upper()[:2]\n","  if subtype == \"H3\":\n","    return h3_index\n","  elif subtype not in df_numbering_scheme.columns:\n","    print(\"Invalid subtype.\")\n","    return -1\n","  else:\n","    res = df_numbering_scheme[df_numbering_scheme['H3'] == h3_index][subtype]\n","    if res.empty:\n","      print(\"Invalid H3 index.\")\n","      return -1\n","    else:\n","      return res.iloc[0]"]},{"cell_type":"code","execution_count":null,"id":"u7rbekIZ_nf1","metadata":{"id":"u7rbekIZ_nf1"},"outputs":[],"source":["def get_min_max(df_numbering_scheme):\n","  min_value = df_numbering_scheme.min().min()\n","  max_value = df_numbering_scheme.max().max()\n","  return min_value, max_value\n"]},{"cell_type":"code","execution_count":null,"id":"fAG60cpkkjCP","metadata":{"id":"fAG60cpkkjCP"},"outputs":[],"source":["#get host from label\n","def get_label(host):\n","  host = host.strip().lower()\n","  if host == 'human':\n","    return 1\n","  else:\n","    return 0"]},{"cell_type":"code","execution_count":null,"id":"rHmaRw05pjsp","metadata":{"collapsed":true,"id":"rHmaRw05pjsp"},"outputs":[],"source":["#getting sequence data as one big dataframe that contains information regarding label (host), year, subtype and sequence\n","df_unprocessed = pd.DataFrame()\n","df_unprocessed = get_df_from_sequences(df_unprocessed, h3_realigned)\n","df_unprocessed = get_df_from_sequences(df_unprocessed, h5_realigned)\n","df_unprocessed = get_df_from_sequences(df_unprocessed, h7_realigned)"]},{"cell_type":"code","execution_count":null,"id":"Heg5BnSXQngY","metadata":{"id":"Heg5BnSXQngY"},"outputs":[],"source":["#method to eliminate duplicate sequences\n","def drop_duplicate_sequences(df):\n","  initial_length = len(df)\n","  df_dropped = df.drop_duplicates(subset=['label', 'subtype', 'sequence'], keep='first')\n","  removed = initial_length - len(df_dropped)\n","  print(f\"{removed} / {initial_length} duplicates removed\")\n","  return df_dropped"]},{"cell_type":"code","execution_count":null,"id":"DYIsn0BHAyv2","metadata":{"id":"DYIsn0BHAyv2"},"outputs":[],"source":["#method to filter sequences for min and max constraints of the mutations\n","def filter_sequences_for_length(df):\n","  min_value, max_value = get_min_max(df_numbering_scheme)\n","  min_value = int(min_value)\n","  max_value = int(max_value)\n","  df['sequence'] = df['sequence'].apply(lambda seq: seq[min_value:max_value+1])\n","  return df"]},{"cell_type":"code","execution_count":null,"id":"XPQzmfly_KL1","metadata":{"id":"XPQzmfly_KL1"},"outputs":[],"source":["#filtering dataframe rows and getting new dataframe for processed sequences\n","df_processed = drop_duplicate_sequences(df_unprocessed)\n","df_processed = filter_sequences_for_length(df_processed)\n"]},{"cell_type":"markdown","id":"97ZTxR3e-t00","metadata":{"id":"97ZTxR3e-t00"},"source":[]},{"cell_type":"markdown","id":"CK7OrhHpd-kO","metadata":{"id":"CK7OrhHpd-kO"},"source":[]},{"cell_type":"code","execution_count":null,"id":"uisq6NbBnKHr","metadata":{"id":"uisq6NbBnKHr"},"outputs":[],"source":["\n","df = df_processed"]},{"cell_type":"code","execution_count":null,"id":"I1fu2KCROHzV","metadata":{"id":"I1fu2KCROHzV"},"outputs":[],"source":["#getting the number of positive and negative labels to ensure an even dataset\n","def get_positive_and_negative(df):\n","  neg_count = 0\n","  pos_count = 0\n","  for i, row in df.iterrows():\n","    if row['label'] == 1 :\n","      pos_count += 1\n","    elif row['label'] == 0:\n","      neg_count += 1\n","    else:\n","      print('Invalid label: ' + row['label'])\n","\n","  print(\"negatives count: \" + str(neg_count))\n","  print(\"positives count: \" + str(pos_count))"]},{"cell_type":"code","execution_count":null,"id":"4SRrpdaKjYgm","metadata":{"id":"4SRrpdaKjYgm"},"outputs":[],"source":["#get logo to better analyse sequences (visually)\n","def get_logo_for_sequences(sequences):\n","  freq_mat = logomaker.alignment_to_matrix(sequences)\n","  freq_mat.head()\n","  logo = logomaker.Logo(freq_mat,\n","               fade_below=0.5,\n","               shade_below=0.5,\n","               color_scheme='skylign_protein',\n","               figsize = (100,5))"]},{"cell_type":"code","execution_count":null,"id":"yLGnTA9Njvgp","metadata":{"id":"yLGnTA9Njvgp"},"outputs":[],"source":["get_logo_for_sequences(df[df['label'] == 0 & df['subtype'].str.contains('H3')]['sequence'])"]},{"cell_type":"code","execution_count":null,"id":"OtGnxwfOkHUX","metadata":{"id":"OtGnxwfOkHUX"},"outputs":[],"source":["get_logo_for_sequences(df[df['label'] == 1 & df['subtype'].str.contains('H3')]['sequence'])"]},{"cell_type":"code","execution_count":null,"id":"qwXcB8IHkboh","metadata":{"id":"qwXcB8IHkboh"},"outputs":[],"source":["get_logo_for_sequences(df[df['label'] == 0 & df['subtype'].str.contains('H5')]['sequence'])"]},{"cell_type":"code","execution_count":null,"id":"7aWAvkMxkfA5","metadata":{"id":"7aWAvkMxkfA5"},"outputs":[],"source":["get_logo_for_sequences(df[df['label'] == 1 & df['subtype'].str.contains('H5')]['sequence'])"]},{"cell_type":"code","execution_count":null,"id":"ZkGhOlqrkg2Q","metadata":{"id":"ZkGhOlqrkg2Q"},"outputs":[],"source":["get_logo_for_sequences(df[df['label'] == 0 & df['subtype'].str.contains('H7')]['sequence'])"]},{"cell_type":"code","execution_count":null,"id":"kPvo73s2kibf","metadata":{"id":"kPvo73s2kibf"},"outputs":[],"source":["get_logo_for_sequences(df[df['label'] == 1 & df['subtype'].str.contains('H7')]['sequence'])"]},{"cell_type":"code","execution_count":null,"id":"u4DEF3v5Jk-O","metadata":{"collapsed":true,"id":"u4DEF3v5Jk-O"},"outputs":[],"source":["#method that encodes all sequences for known mutations (multiple hot encoding)\n","def encode_sequences(df, mutations_df):\n","\n","  feature_names = list()\n","  encoded_sequences = []\n","  for _, row in df.iterrows():\n","\n","    sequence = row['sequence']\n","    #get first two letter of subtype to infer HX\n","    subtype = row['subtype']\n","    sequence_features = [0] * len(mutations_df)\n","    index = 0\n","\n","    min, _ = get_min_max(df_numbering_scheme)\n","    min = min.astype(int)\n","\n","    for i, mutation_row in mutations_df.iterrows():\n","      original, mutated, H3_index = mutation_row\n","      feature_names.append(f\"{original}_to_{mutated}_at_:{H3_index}\")\n","\n","      subtype_index = get_subtype_index(subtype, H3_index) - min\n","\n","      #handle mutations that state 'X or Y' or 'Any'\n","      if 'or' in mutated:\n","            allowed_mutations = mutated.split('or')\n","            if sequence[subtype_index] in allowed_mutations:\n","              sequence_features[i] = 1\n","      elif mutated == 'Any':\n","            sequence_features[i] = 1\n","      elif sequence[subtype_index] == mutated:\n","            sequence_features[i] = 1\n","\n","    encoded_sequences.append(tuple(sequence_features))\n","\n","  return feature_names, encoded_sequences"]},{"cell_type":"code","execution_count":null,"id":"yifNbz7iag7X","metadata":{"collapsed":true,"id":"yifNbz7iag7X"},"outputs":[],"source":["#getting feature names and matrices for each sequence in df\n","feature_names, encoded_sequences = encode_sequences(df, known_mutations_df)"]},{"cell_type":"code","execution_count":null,"id":"vyGXhwW8PCs5","metadata":{"collapsed":true,"id":"vyGXhwW8PCs5"},"outputs":[],"source":["#storing feature matrices as new column\n","df['encoded sequences'] = encoded_sequences"]},{"cell_type":"code","execution_count":null,"id":"R6tbo8LSEuVM","metadata":{"collapsed":true,"id":"R6tbo8LSEuVM"},"outputs":[],"source":["#visualize feature relations in correlation matrix\n","data = np.vstack(df['encoded sequences'].apply(np.array))\n","\n","corr_matrix = pd.DataFrame(data, columns=[known_mutations_df['H3 index']]).corr()\n","\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(corr_matrix, annot=True, annot_kws={\"size\": 5}, cmap='coolwarm', fmt='.2f')\n","plt.title('Correlation between Mutations')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"bed2077c-822f-40fa-8a89-2b24baaaaa1e","metadata":{"id":"bed2077c-822f-40fa-8a89-2b24baaaaa1e"},"outputs":[],"source":["#method for splitting the data into training and testing sets based on input percentage (for the test set)\n","def split_data(dataframe, percentage):\n","    df_shuffled = dataframe.sample(frac=1, random_state=42).reset_index(drop=True)\n","    return jackknife_split(df_shuffled, percentage)\n","#method applying the jackknife split\n","def jackknife_split(data, percentage):\n","    test_size = int((1 - percentage) * len(data))\n","    test_data = data.iloc[:-test_size]\n","    train_data = data.iloc[-test_size:]\n","    return train_data, test_data\n","\n"]},{"cell_type":"code","source":["\n","#use SMOTE to oversample synthetic data, balancing h3, h5, and h7 training data\n","def smote_balance_data(h3_data, h5_data, h7_data):\n","  target_count = len(h3_data)\n","\n","  x_h5 = np.array(h5_data['encoded sequences'].tolist())\n","  y_h5 = np.array(h5_data['label'].tolist())\n","\n","  x_h7 = np.array(h7_data['encoded sequences'].tolist())\n","  y_h7 = np.array(h7_data['label'].tolist())\n","\n","  smote = SMOTE(random_state=42)\n","\n","  x_h5_resampled, y_h5_resampled = smote.fit_resample(x_h5, y_h5)\n","\n","  x_h5_resampled = x_h5_resampled[:target_count]\n","  y_h5_resampled = y_h5_resampled[:target_count]\n","\n","  x_h7_resampled, y_h7_resampled = smote.fit_resample(x_h7, y_h7)\n","\n","  x_h7_resampled = x_h7_resampled[:target_count]\n","  y_h7_resampled = y_h7_resampled[:target_count]\n","\n","  h5_resampled = pd.DataFrame({\n","      'encoded sequences': list(x_h5_resampled),\n","      'label': y_h5_resampled,\n","      'subtype': ['H5'] * len(y_h5_resampled)\n","  })\n","\n","  h7_resampled = pd.DataFrame({\n","      'encoded sequences': list(x_h7_resampled),\n","      'label': y_h7_resampled,\n","      'subtype': ['H7'] * len(y_h7_resampled)\n","  })\n","\n","  balanced_data = pd.concat([h3_data, h5_resampled, h7_resampled], ignore_index=True)\n","\n","\n","\n","  return balanced_data\n","\n"],"metadata":{"id":"BDIFiBykSiJF"},"id":"BDIFiBykSiJF","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"c9653bfd","metadata":{"id":"c9653bfd"},"outputs":[],"source":["# forming the training and testing datasets\n","h3_train = df[(df['subtype'].str.contains('H3'))]\n","h5_train, h5_test = split_data(df[(df['subtype'].str.contains('H5'))], 0.2)\n","h7_train, h7_test = split_data(df[(df['subtype'].str.contains('H7'))], 0.2)\n","\n","train_data = smote_balance_data(h3_train, h5_train, h7_train)\n","\n","test_data = pd.concat([h5_test, h7_test])\n","\n","\n","x_train = np.array(train_data['encoded sequences'].tolist())\n","y_train = np.array(train_data[\"label\"].tolist())\n","x_test = np.array(test_data['encoded sequences'].tolist())\n","y_test = np.array(test_data[\"label\"]).tolist()\n"]},{"cell_type":"code","source":["print(\"Number of training sequences: \" + str(len(train_data)))\n","print(\"Number of testing sequences: \" + str(len(test_data)))"],"metadata":{"id":"JjOCK_sjj7Q9"},"id":"JjOCK_sjj7Q9","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"TKGRtVGeEtZp","metadata":{"id":"TKGRtVGeEtZp"},"outputs":[],"source":["#get names of poly features based on mutations\n","def get_poly_feature_names(poly, x_train, feature_names):\n","  original_feature_names = [feature_names[i] for i in range(x_train.shape[1])]\n","  poly_feature_names = poly.get_feature_names_out(original_feature_names)\n","  poly_feature_names = [name.replace(' ', '&').replace('.', '&') for name in poly_feature_names]\n","  return pd.DataFrame({'feature' : x_train_poly[1], 'name': poly_feature_names})"]},{"cell_type":"code","execution_count":null,"id":"4igg8ABKDqTK","metadata":{"id":"4igg8ABKDqTK"},"outputs":[],"source":["#transform data\n","poly = PolynomialFeatures(degree=3)\n","x_train_poly = poly.fit_transform(x_train)\n","x_test_poly = poly.transform(x_test)\n","poly_feature_names = get_poly_feature_names(poly, x_train, feature_names)"]},{"cell_type":"code","execution_count":null,"id":"1qQSgTFEG3H7","metadata":{"id":"1qQSgTFEG3H7"},"outputs":[],"source":["#method for selection of poly features with p > 0.05\n","def select_from_poly(poly_features, x_train_poly, y_train, alpha=0.05, score_func=f_classif):\n","  selector = SelectFpr(score_func=score_func, alpha= alpha)\n","  selector.fit(x_train_poly, y_train)\n","  return selector\n"]},{"cell_type":"markdown","id":"XFhzHw4qi9ZJ","metadata":{"id":"XFhzHw4qi9ZJ"},"source":[]},{"cell_type":"code","execution_count":null,"id":"ghHN0By4mFAW","metadata":{"id":"ghHN0By4mFAW"},"outputs":[],"source":["#select and transform features\n","selector = select_from_poly(poly_feature_names, x_train_poly, y_train)\n","x_train = selector.transform(x_train_poly)\n","x_test = selector.transform(x_test_poly)"]},{"cell_type":"code","execution_count":null,"id":"higDjLg-pZjm","metadata":{"collapsed":true,"id":"higDjLg-pZjm"},"outputs":[],"source":["#view selected features\n","selected_features = selector.get_support()\n","selected_feature_names = poly_feature_names[selected_features]\n","selected_feature_names.reset_index()"]},{"cell_type":"code","source":["#construct logistic regression model\n","log_reg = LogisticRegression(random_state=42)\n","param_grid = {\n","    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n","    'penalty': ['l2'],\n","    'solver': ['lbfgs', 'newton-cholesky'],\n","    'max_iter': [1000, 2000],\n","    'class_weight': ['balanced']\n","}"],"metadata":{"id":"RtR3QRg-_cjD"},"id":"RtR3QRg-_cjD","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"896edde2","metadata":{"id":"896edde2"},"outputs":[],"source":["#construct linear svc model\n","svm = SVC(kernel='linear', probability=True,  class_weight='balanced', decision_function_shape= 'ovr', random_state= 42)\n","param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n","              'tol': [0.0001, 0.001, 0.01],\n","              'max_iter': [5000],\n","             }\n"]},{"cell_type":"code","execution_count":null,"id":"D74wMmAHuznB","metadata":{"collapsed":true,"id":"D74wMmAHuznB"},"outputs":[],"source":["#construct random forest model\n","rf = RandomForestClassifier(random_state=42, class_weight = 'balanced')\n","\n","param_grid = {\n","    'n_estimators': [100, 200, 300],\n","    'max_depth': [3, 5, 7],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [1, 2, 4],\n","}\n"]},{"cell_type":"code","execution_count":null,"id":"bO7pU__Z_6LS","metadata":{"id":"bO7pU__Z_6LS"},"outputs":[],"source":["#construct xgboost model\n","xgb_clf = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n","\n","param_grid = {\n","    'booster': ['gbtree', 'dart'],\n","    'n_jobs': [-1],\n","    'n_estimators': [200],\n","    'max_depth': [5, 7],\n","    'learning_rate': [0.05],\n","    'subsample': [0.7],\n","    'colsample_bytree': [0.7],\n","    'min_child_weight': [3],\n","    'gamma': [0.1, 0.2]\n","}\n"]},{"cell_type":"code","execution_count":null,"id":"urLZ7jyaQdmt","metadata":{"id":"urLZ7jyaQdmt"},"outputs":[],"source":["#method to train and evaluate given model using gridsearch\n","def train_and_evaluate_model(selected_model, param_grid, x_train, y_train, x_test, y_test):\n","  grid_search = GridSearchCV(selected_model, param_grid, cv=5, scoring='f1_weighted')\n","\n","  grid_search.fit(x_train, y_train)\n","\n","  model = grid_search.best_estimator_\n","\n","  y_pred = model.predict(x_test)\n","\n","  y_probs = model.predict_proba(x_test)[:, 1]\n","\n","  print(f\"Best Validation Score: {grid_search.best_score_}\")\n","\n","  return model, y_pred, y_probs\n"]},{"cell_type":"code","execution_count":null,"id":"1WSAHsMOys9P","metadata":{"id":"1WSAHsMOys9P"},"outputs":[],"source":["#method to print classification report of a model\n","def get_classification_report(y_test, y_pred):\n","  class_report = classification_report(y_test, y_pred, digits=4)\n","  print(\"Classification Report:\\n\", class_report)"]},{"cell_type":"code","execution_count":null,"id":"hqtj_8gCIXSa","metadata":{"collapsed":true,"id":"hqtj_8gCIXSa"},"outputs":[],"source":["#train desired model (inputted: xgboost)\n","model, y_pred, y_probs = train_and_evaluate_model(xgb_clf, param_grid, x_train, y_train, x_test, y_test)"]},{"cell_type":"code","execution_count":null,"id":"cg4VumsONDYG","metadata":{"id":"cg4VumsONDYG"},"outputs":[],"source":["#get predictive metrics\n","get_classification_report(y_test, y_pred)"]},{"cell_type":"code","execution_count":null,"id":"fH1Hw4ljNZ4I","metadata":{"id":"fH1Hw4ljNZ4I"},"outputs":[],"source":["#method to plot precision-recall\n","def get_precision_recall_curve(y_test, y_probs):\n","  precision, recall, thresholds = precision_recall_curve(y_test, y_probs)\n","\n","  # Plot Precision-Recall curve\n","  plt.plot(recall, precision, marker='.')\n","  plt.xlabel('Recall')\n","  plt.ylabel('Precision')\n","  plt.title('Precision-Recall Curve')\n","  plt.show()"]},{"cell_type":"code","execution_count":null,"id":"F2HrK9oMaNO8","metadata":{"id":"F2HrK9oMaNO8"},"outputs":[],"source":["get_precision_recall_curve(y_test, y_probs)"]},{"cell_type":"code","execution_count":null,"id":"KKLxn97GOCWO","metadata":{"id":"KKLxn97GOCWO"},"outputs":[],"source":["#method to plot roc and calculate auc\n","def get_roc_curve(y_test, y_probs):\n","  # Compute ROC curve\n","  fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n","\n","  # Compute AUC\n","  auc = roc_auc_score(y_test, y_probs)\n","  print(f\"AUC: {auc:.2f}\")\n","\n","  # Plot ROC curve\n","  plt.figure(figsize=(8, 6))\n","  plt.plot(fpr, tpr, color='blue', label=f'AUC = {auc:.2f}')\n","  plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n","  plt.xlabel('False Positive Rate')\n","  plt.ylabel('True Positive Rate')\n","  plt.title('ROC Curve')\n","  plt.legend()\n","  plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"tm_0KUWuyqlX","metadata":{"id":"tm_0KUWuyqlX"},"outputs":[],"source":["get_roc_curve(y_test, y_probs)"]},{"cell_type":"code","execution_count":null,"id":"1798E2xtNx6W","metadata":{"collapsed":true,"id":"1798E2xtNx6W"},"outputs":[],"source":["#method to print confusion matrix showing TP, TN, FN & FP values\n","def get_conf_matrix(y_test, y_pred):\n","  conf_matrix = confusion_matrix(y_test, y_pred)\n","  print(\"Confusion Matrix:\\n\")\n","  print(\"True Positives: \" + str(conf_matrix[0][0]))\n","  print(\"False Positives: \" + str(conf_matrix[0][1]))\n","  print(\"False Negatives: \" + str(conf_matrix[1][0]))\n","  print(\"True Negatives: \" + str(conf_matrix[1][1]))\n","  return conf_matrix\n","\n"]},{"cell_type":"code","execution_count":null,"id":"puDgK1ImOTaZ","metadata":{"id":"puDgK1ImOTaZ"},"outputs":[],"source":["cm = get_conf_matrix(y_test, y_pred)"]},{"cell_type":"code","execution_count":null,"id":"qb7PW7FGVE3m","metadata":{"id":"qb7PW7FGVE3m"},"outputs":[],"source":["#method to plot the confusion matrix into a heatmap for better visualization\n","def plot_conf_matrix(cm):\n","  cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","  plt.figure(figsize=(8, 6))\n","  sns.heatmap(cm_normalized, annot=True, fmt=\".4f\", cmap=\"Blues\", cbar=False)\n","  plt.ylabel('True label')\n","  plt.xlabel('Predicted label')\n","  plt.title('Normalized Confusion Matrix')\n","  plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"JZ0xSTKbVNbj","metadata":{"id":"JZ0xSTKbVNbj"},"outputs":[],"source":["plot_conf_matrix(cm)"]},{"cell_type":"code","source":["#method to get weights of a model\n","def get_coeffs(model):\n","  if hasattr(model, 'coef_'):\n","    return model.coef_[0]\n","  elif hasattr(model, 'feature_importances_'):\n","    return model.feature_importances_"],"metadata":{"id":"B5DLEsyZYS1P"},"id":"B5DLEsyZYS1P","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"mlumM7qNSSL6","metadata":{"collapsed":true,"id":"mlumM7qNSSL6"},"outputs":[],"source":["#print final features and their determined coefficients\n","features_to_investigate = []\n","for i in range(0, len(get_coeffs(model))):\n","  if (abs(float(get_coeffs(model)[i])) > 0.0000):\n","    features_to_investigate.append(selected_feature_names.iloc[i]['name'])\n","  print(\"Feature :\" +  str(selected_feature_names.iloc[i]['name']) + \", Coeff: \" + f\"{(get_coeffs(model)[i]):.3f}\")\n"]},{"cell_type":"markdown","id":"A4lF0G3lzE1V","metadata":{"id":"A4lF0G3lzE1V"},"source":[]},{"cell_type":"code","execution_count":null,"id":"8WG_BWm2AZgN","metadata":{"id":"8WG_BWm2AZgN"},"outputs":[],"source":["#get py3dmol view of a pdb protein\n","def get_protein_view(pdb_file_name, color = 'lightgrey'):\n","  pdb_data = \"\"\n","  with open(os.path.join(pdb_directory, pdb_file_name + \".pdb\"), 'r') as file:\n","    pdb_data = file.read()\n","  view = py3Dmol.view()\n","  view.addModel(pdb_data, 'pdb')\n","  view.setStyle({'cartoon': {'color': color }})\n","  return view"]},{"cell_type":"code","execution_count":null,"id":"AeUTI5M8nI0Z","metadata":{"id":"AeUTI5M8nI0Z"},"outputs":[],"source":["def normalize(value, min_value, max_value):\n","    if max_value == min_value:\n","        min_value = -1\n","        max_value = 1\n","    return (value - min_value) / (max_value - min_value)"]},{"cell_type":"code","execution_count":null,"id":"b-Fl3--oZPvY","metadata":{"id":"b-Fl3--oZPvY"},"outputs":[],"source":["#method to visualize antigenic regions on a h3n2 human protein\n","def highlight_antigenic_regions(view):\n","  antigenic_sites = {\n","    \"A\": [(122, 130), (154, 160)],\n","    \"B\": [(155, 165), (189, 196)],\n","    \"C\": [(50, 58)],\n","    \"D\": [(160, 167), (200, 207)],\n","    \"E\": [(77, 83)],\n","    \"F\": [(220, 230)],\n","  }\n","\n","  site_colors = {\n","    \"A\": \"purple\",\n","    \"B\": \"black\",\n","    \"C\": \"green\",\n","    \"D\": \"yellow\",\n","    \"E\": \"magenta\",\n","    \"F\": \"orange\"\n","  }\n","\n","  for site, ranges in antigenic_sites.items():\n","    for res_range in ranges:\n","        start_res, end_res = res_range\n","        selection = f\"resi {start_res}-{end_res}\"  # Select the residue range\n","        view.addStyle({'chain': 'A', 'resi': list(range(start_res, end_res + 1))},  # Apply style to these residues\n","                      {'cartoon': {'color': site_colors[site]}})  # Apply color\n","  return view\n"]},{"cell_type":"code","execution_count":null,"id":"GyBGb0cRnX3o","metadata":{"id":"GyBGb0cRnX3o"},"outputs":[],"source":["#method to visualize mutation residues\n","def visualize_mutations(view, mutations):\n","    for res in mutations:\n","              view.addStyle({'resi': [res]} , {'stick': {'color': 'pink'}})\n","              view.zoomTo()\n","\n","    return view\n"]},{"cell_type":"code","source":["#method to highlight HA1, HA2, and RBS regions of a h3n2 human HA protein\n","def highlight_regions(view):\n","  # Define residue ranges for HA1, HA2, and RBS regions\n","  ha1_range = [(1, 329)]\n","  ha2_range = [(330, 566)]\n","  rbs_ranges = [(98,100), (130,133), (220,223)]  # Example RBS range\n","\n","\n","  for ranges in ha1_range:\n","      view.addStyle({'chain': 'A', 'resi': list(range(ranges[0], ranges[1] + 1))}, {'cartoon': {'color': 'blue'}})\n","\n","  for ranges in ha2_range:\n","        view.addStyle({'chain': 'A', 'resi': list(range(ranges[0], ranges[1] + 1))}, {'cartoon': {'color': 'red'}})\n","\n","  # Highlight RBS in green\n","  for ranges in rbs_ranges:\n","        view.addStyle({'chain': 'A', 'resi': list(range(ranges[0], ranges[1] + 1))}, {'cartoon': {'color': 'green'}})\n","\n","  return view\n"],"metadata":{"id":"YyicmWspSd-8"},"id":"YyicmWspSd-8","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"mY9X2sMyFh5U","metadata":{"id":"mY9X2sMyFh5U"},"outputs":[],"source":["#layer all viewings together or visualize one at a time by commenting lines out\n","view = get_protein_view(\"h3n2_human\")\n","view_linear = visualize_mutations(view, [192, 226, 227])\n","view_linear = highlight_antigenic_regions(view_linear)\n","view_linear = highlight_regions(view_linear)\n","view_linear.show()"]},{"cell_type":"code","source":["\n","#method to visualize data overview\n","def plot_subtype_distribution(df, title):\n","  \"\"\"\n","  Plots the subtype distribution for a given DataFrame, colored by host.\n","  \"\"\"\n","  subtype_counts = df.groupby(['subtype', 'label'])['label'].count().unstack().fillna(0)\n","  subtype_counts_percent = subtype_counts.div(subtype_counts.sum(axis=1), axis=0) * 100\n","\n","  plt.figure(figsize=(12, 6))\n","  ax = subtype_counts_percent.plot(kind='bar', stacked=True, color=['skyblue', 'salmon'])\n","\n","  plt.title(title)\n","  plt.xlabel('Subtype')\n","  plt.ylabel('Percentage')\n","  plt.xticks(rotation=45, ha='right')\n","  plt.legend(title='Host', labels=['Avian', 'Human'])\n","\n","  # Annotate bars with percentages\n","  for p in ax.patches:\n","    width = p.get_width()\n","    height = p.get_height()\n","    x, y = p.get_xy()\n","    ax.annotate(f'{height:.1f}%', (x + width / 2, y + height / 2), ha='center', va='center')\n","\n","  plt.show()\n","\n","# Plot subtype distribution for training data\n","plot_subtype_distribution(train_data.assign(subtype=train_data['subtype'].str[:2]), 'Host Distribution in Training Data (Per Subtype)')\n","\n","# Plot subtype distribution for test data\n","plot_subtype_distribution(test_data.assign(subtype=test_data['subtype'].str[:2]), 'Host Distribution in Test Data (Per Subtype)')\n"],"metadata":{"id":"Ej-7Iv68asXo"},"id":"Ej-7Iv68asXo","execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_results = {\n","    \"Logistic Regression\": \"\"\"\n","        Feature :H_to_Y_at_:110, Coeff: -0.013\n","Feature :I_to_T_at_:155, Coeff: 0.018\n","Feature :T_to_I_at_:192, Coeff: 0.243\n","Feature :Q_to_L_at_:226, Coeff: 0.049\n","Feature :S_to_N_at_:227, Coeff: 0.036\n","Feature :G_to_S_at_:228, Coeff: 0.004\n","Feature :H_to_Y_at_:110^2, Coeff: -0.013\n","Feature :H_to_Y_at_:110&N_to_K_at_:186, Coeff: -0.012\n","Feature :H_to_Y_at_:110&T_to_I_at_:192, Coeff: -0.013\n","Feature :I_to_T_at_:155^2, Coeff: 0.018\n","Feature :I_to_T_at_:155&Q_to_L_at_:226, Coeff: 0.018\n","Feature :T_to_I_at_:192^2, Coeff: 0.243\n","Feature :Q_to_L_at_:226^2, Coeff: 0.049\n","Feature :Q_to_L_at_:226&P_to_S_at_:239, Coeff: -0.002\n","Feature :S_to_N_at_:227^2, Coeff: 0.036\n","Feature :G_to_S_at_:228^2, Coeff: 0.004\n","Feature :H_to_Y_at_:110^3, Coeff: -0.013\n","Feature :H_to_Y_at_:110^2&N_to_K_at_:186, Coeff: -0.012\n","Feature :H_to_Y_at_:110^2&T_to_I_at_:192, Coeff: -0.013\n","Feature :H_to_Y_at_:110&N_to_K_at_:186^2, Coeff: -0.012\n","Feature :H_to_Y_at_:110&N_to_K_at_:186&T_to_I_at_:192, Coeff: -0.012\n","Feature :H_to_Y_at_:110&T_to_I_at_:192^2, Coeff: -0.013\n","Feature :I_to_T_at_:155^3, Coeff: 0.018\n","Feature :I_to_T_at_:155^2&Q_to_L_at_:226, Coeff: 0.018\n","Feature :I_to_T_at_:155&Q_to_L_at_:226^2, Coeff: 0.018\n","Feature :T_to_I_at_:192^3, Coeff: 0.243\n","Feature :Q_to_L_at_:226^3, Coeff: 0.049\n","Feature :Q_to_L_at_:226^2&P_to_S_at_:239, Coeff: -0.002\n","Feature :Q_to_L_at_:226&P_to_S_at_:239^2, Coeff: -0.002\n","Feature :S_to_N_at_:227^3, Coeff: 0.036\n","Feature :G_to_S_at_:228^3, Coeff: 0.004\n","    \"\"\",\n","    \"SVC with Linear Kernel\": \"\"\"\n","       Feature :H_to_Y_at_:110, Coeff: 0.000\n","Feature :I_to_T_at_:155, Coeff: 0.000\n","Feature :T_to_I_at_:192, Coeff: 0.667\n","Feature :Q_to_L_at_:226, Coeff: -0.222\n","Feature :S_to_N_at_:227, Coeff: -0.444\n","Feature :G_to_S_at_:228, Coeff: 0.173\n","Feature :H_to_Y_at_:110^2, Coeff: 0.000\n","Feature :H_to_Y_at_:110&N_to_K_at_:186, Coeff: 0.000\n","Feature :H_to_Y_at_:110&T_to_I_at_:192, Coeff: 0.000\n","Feature :I_to_T_at_:155^2, Coeff: 0.000\n","Feature :I_to_T_at_:155&Q_to_L_at_:226, Coeff: 0.000\n","Feature :T_to_I_at_:192^2, Coeff: 0.667\n","Feature :Q_to_L_at_:226^2, Coeff: -0.222\n","Feature :Q_to_L_at_:226&P_to_S_at_:239, Coeff: -0.222\n","Feature :S_to_N_at_:227^2, Coeff: -0.444\n","Feature :G_to_S_at_:228^2, Coeff: 0.173\n","Feature :H_to_Y_at_:110^3, Coeff: 0.000\n","Feature :H_to_Y_at_:110^2&N_to_K_at_:186, Coeff: 0.000\n","Feature :H_to_Y_at_:110^2&T_to_I_at_:192, Coeff: 0.000\n","Feature :H_to_Y_at_:110&N_to_K_at_:186^2, Coeff: 0.000\n","Feature :H_to_Y_at_:110&N_to_K_at_:186&T_to_I_at_:192, Coeff: 0.000\n","Feature :H_to_Y_at_:110&T_to_I_at_:192^2, Coeff: 0.000\n","Feature :I_to_T_at_:155^3, Coeff: 0.000\n","Feature :I_to_T_at_:155^2&Q_to_L_at_:226, Coeff: 0.000\n","Feature :I_to_T_at_:155&Q_to_L_at_:226^2, Coeff: 0.000\n","Feature :T_to_I_at_:192^3, Coeff: 0.667\n","Feature :Q_to_L_at_:226^3, Coeff: -0.222\n","Feature :Q_to_L_at_:226^2&P_to_S_at_:239, Coeff: -0.222\n","Feature :Q_to_L_at_:226&P_to_S_at_:239^2, Coeff: -0.222\n","Feature :S_to_N_at_:227^3, Coeff: -0.444\n","Feature :G_to_S_at_:228^3, Coeff: 0.173\n","    \"\"\",\n","    \"Random Forest\": \"\"\"\n","        Feature :H_to_Y_at_:110, Coeff: 0.006\n","Feature :I_to_T_at_:155, Coeff: 0.007\n","Feature :T_to_I_at_:192, Coeff: 0.189\n","Feature :Q_to_L_at_:226, Coeff: 0.042\n","Feature :S_to_N_at_:227, Coeff: 0.058\n","Feature :G_to_S_at_:228, Coeff: 0.005\n","Feature :H_to_Y_at_:110^2, Coeff: 0.004\n","Feature :H_to_Y_at_:110&N_to_K_at_:186, Coeff: 0.002\n","Feature :H_to_Y_at_:110&T_to_I_at_:192, Coeff: 0.004\n","Feature :I_to_T_at_:155^2, Coeff: 0.008\n","Feature :I_to_T_at_:155&Q_to_L_at_:226, Coeff: 0.006\n","Feature :T_to_I_at_:192^2, Coeff: 0.227\n","Feature :Q_to_L_at_:226^2, Coeff: 0.038\n","Feature :Q_to_L_at_:226&P_to_S_at_:239, Coeff: 0.001\n","Feature :S_to_N_at_:227^2, Coeff: 0.054\n","Feature :G_to_S_at_:228^2, Coeff: 0.006\n","Feature :H_to_Y_at_:110^3, Coeff: 0.004\n","Feature :H_to_Y_at_:110^2&N_to_K_at_:186, Coeff: 0.001\n","Feature :H_to_Y_at_:110^2&T_to_I_at_:192, Coeff: 0.008\n","Feature :H_to_Y_at_:110&N_to_K_at_:186^2, Coeff: 0.002\n","Feature :H_to_Y_at_:110&N_to_K_at_:186&T_to_I_at_:192, Coeff: 0.001\n","Feature :H_to_Y_at_:110&T_to_I_at_:192^2, Coeff: 0.004\n","Feature :I_to_T_at_:155^3, Coeff: 0.008\n","Feature :I_to_T_at_:155^2&Q_to_L_at_:226, Coeff: 0.011\n","Feature :I_to_T_at_:155&Q_to_L_at_:226^2, Coeff: 0.005\n","Feature :T_to_I_at_:192^3, Coeff: 0.194\n","Feature :Q_to_L_at_:226^3, Coeff: 0.040\n","Feature :Q_to_L_at_:226^2&P_to_S_at_:239, Coeff: 0.001\n","Feature :Q_to_L_at_:226&P_to_S_at_:239^2, Coeff: 0.002\n","Feature :S_to_N_at_:227^3, Coeff: 0.056\n","Feature :G_to_S_at_:228^3, Coeff: 0.005\n","    \"\"\",\n","\n","    \"XGBoost\": \"\"\"\n","    Feature :H_to_Y_at_:110, Coeff: 0.052\n","Feature :I_to_T_at_:155, Coeff: 0.041\n","Feature :T_to_I_at_:192, Coeff: 0.142\n","Feature :Q_to_L_at_:226, Coeff: 0.040\n","Feature :S_to_N_at_:227, Coeff: 0.117\n","Feature :G_to_S_at_:228, Coeff: 0.000\n","Feature :H_to_Y_at_:110^2, Coeff: 0.018\n","Feature :H_to_Y_at_:110&N_to_K_at_:186, Coeff: 0.000\n","Feature :H_to_Y_at_:110&T_to_I_at_:192, Coeff: 0.047\n","Feature :I_to_T_at_:155^2, Coeff: 0.028\n","Feature :I_to_T_at_:155&Q_to_L_at_:226, Coeff: 0.045\n","Feature :T_to_I_at_:192^2, Coeff: 0.200\n","Feature :Q_to_L_at_:226^2, Coeff: 0.036\n","Feature :Q_to_L_at_:226&P_to_S_at_:239, Coeff: 0.000\n","Feature :S_to_N_at_:227^2, Coeff: 0.058\n","Feature :G_to_S_at_:228^2, Coeff: 0.000\n","Feature :H_to_Y_at_:110^3, Coeff: 0.000\n","Feature :H_to_Y_at_:110^2&N_to_K_at_:186, Coeff: 0.000\n","Feature :H_to_Y_at_:110^2&T_to_I_at_:192, Coeff: 0.000\n","Feature :H_to_Y_at_:110&N_to_K_at_:186^2, Coeff: 0.000\n","Feature :H_to_Y_at_:110&N_to_K_at_:186&T_to_I_at_:192, Coeff: 0.000\n","Feature :H_to_Y_at_:110&T_to_I_at_:192^2, Coeff: 0.000\n","Feature :I_to_T_at_:155^3, Coeff: 0.030\n","Feature :I_to_T_at_:155^2&Q_to_L_at_:226, Coeff: 0.000\n","Feature :I_to_T_at_:155&Q_to_L_at_:226^2, Coeff: 0.000\n","Feature :T_to_I_at_:192^3, Coeff: 0.060\n","Feature :Q_to_L_at_:226^3, Coeff: 0.045\n","Feature :Q_to_L_at_:226^2&P_to_S_at_:239, Coeff: 0.000\n","Feature :Q_to_L_at_:226&P_to_S_at_:239^2, Coeff: 0.000\n","Feature :S_to_N_at_:227^3, Coeff: 0.042\n","Feature :G_to_S_at_:228^3, Coeff: 0.000\n","\"\"\"\n","\n","}\n","\n","# Function to parse the model results\n","def parse_model_results(model_results):\n","    data = []\n","\n","    for model_name, results in model_results.items():\n","        # Extract features and coefficients using regex\n","        for line in results.strip().split('\\n'):\n","            match = re.search(r'Feature :(.+), Coeff: (.+)', line)\n","            if match:\n","                feature_name = match.group(1).strip()\n","                coeff = float(match.group(2).strip())\n","\n","                # Only include features with non-zero coefficients\n","                if coeff != 0:\n","                    data.append((model_name, feature_name, coeff))\n","\n","    return pd.DataFrame(data, columns=[\"Model\", \"Feature\", \"Coefficient\"])\n","\n","# Create the DataFrame\n","importance_df = parse_model_results(model_results)\n"],"metadata":{"id":"8_F-bAvgjfz-"},"id":"8_F-bAvgjfz-","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#compare feature importances for linear and tree-based models\n","comparison_values = []\n","\n","for index, row in importance_df.iterrows():\n","\n","    # Extract the feature importances or coefficients for the current model\n","    model_importances = importance_df[importance_df['Model'] == row['Model']]['Coefficient'].values\n","\n","    # Check if the model is linear (Logistic Regression or SVC with linear kernel)\n","    if row['Model'] in ['Logistic Regression', 'SVM with Linear Kernel']:\n","        # Take the absolute values of the coefficients\n","        model_importances = abs(model_importances)\n","\n","    # Normalize the importances/coefficients\n","    normalized = model_importances / np.sum(model_importances)\n","\n","    # Append the normalized value for the current feature to the list\n","    model_specific_index = index - importance_df[importance_df['Model'] == row['Model']].index[0]\n","    comparison_values.append(normalized[model_specific_index])\n","\n","# Add the new comparison column to the DataFrame\n","importance_df['comparison'] = comparison_values\n"],"metadata":{"id":"tt_9p7psG8qk"},"id":"tt_9p7psG8qk","execution_count":null,"outputs":[]},{"cell_type":"code","source":["visualize the results of importance_df for each model\n","\n","plt.figure(figsize=(12, 6))\n","sns.barplot(x='Feature', y='comparison', hue='Model', data=importance_df)\n","plt.xticks(rotation=90)\n","plt.title('Feature Importance Comparison Across Models')\n","plt.xlabel('Feature')\n","plt.ylabel('Importance (Normalized)')\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"8nROncw4JjhT"},"id":"8nROncw4JjhT","execution_count":null,"outputs":[]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[{"file_id":"1wYWvJR7LF1OkYvmTsK3rH7DvyXa3fM0e","timestamp":1716160390913}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.19"}},"nbformat":4,"nbformat_minor":5}